<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Unit 11: Linear Mixed-Effects Models: Longitudinal Analysis | EPsy 8252 Notes</title>
  <meta name="description" content="These are the notes for EPsy 8252.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Unit 11: Linear Mixed-Effects Models: Longitudinal Analysis | EPsy 8252 Notes" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="These are the notes for EPsy 8252." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Unit 11: Linear Mixed-Effects Models: Longitudinal Analysis | EPsy 8252 Notes" />
  
  <meta name="twitter:description" content="These are the notes for EPsy 8252." />
  

<meta name="author" content="Andrew Zieffler">


<meta name="date" content="2019-03-08">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="lmer-assumptions.html">
<link rel="next" href="data-codebook.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="print.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">EPsy 8252 Notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="rmarkdown.html"><a href="rmarkdown.html"><i class="fa fa-check"></i><b>1</b> R Markdown</a><ul>
<li class="chapter" data-level="" data-path="rmarkdown.html"><a href="rmarkdown.html#preparation"><i class="fa fa-check"></i>Preparation</a></li>
<li class="chapter" data-level="1.1" data-path="rmarkdown.html"><a href="rmarkdown.html#notes"><i class="fa fa-check"></i><b>1.1</b> Notes</a></li>
<li class="chapter" data-level="1.2" data-path="rmarkdown.html"><a href="rmarkdown.html#other-resources"><i class="fa fa-check"></i><b>1.2</b> Other Resources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="pretty-printing-tables-in-markdown.html"><a href="pretty-printing-tables-in-markdown.html"><i class="fa fa-check"></i>Pretty-Printing Tables in Markdown</a><ul>
<li class="chapter" data-level="" data-path="pretty-printing-tables-in-markdown.html"><a href="pretty-printing-tables-in-markdown.html#summary-statistics-table"><i class="fa fa-check"></i>Summary Statistics Table</a></li>
<li class="chapter" data-level="" data-path="pretty-printing-tables-in-markdown.html"><a href="pretty-printing-tables-in-markdown.html#correlation-table"><i class="fa fa-check"></i>Correlation Table</a></li>
<li class="chapter" data-level="" data-path="pretty-printing-tables-in-markdown.html"><a href="pretty-printing-tables-in-markdown.html#regression-table-single-model"><i class="fa fa-check"></i>Regression Table: Single Model</a></li>
<li class="chapter" data-level="" data-path="pretty-printing-tables-in-markdown.html"><a href="pretty-printing-tables-in-markdown.html#regression-table-multiple-models"><i class="fa fa-check"></i>Regression Table: Multiple Models</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html"><i class="fa fa-check"></i><b>2</b> Nonlinearity: Log-Transforming the Predictor</a><ul>
<li class="chapter" data-level="" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#preparation-1"><i class="fa fa-check"></i>Preparation</a></li>
<li class="chapter" data-level="2.1" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#dataset-and-research-question"><i class="fa fa-check"></i><b>2.1</b> Dataset and Research Question</a></li>
<li class="chapter" data-level="2.2" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#log-transformation-of-a-variable"><i class="fa fa-check"></i><b>2.2</b> Log-Transformation of a Variable</a><ul>
<li class="chapter" data-level="2.2.1" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#quick-refresher-on-logarithms"><i class="fa fa-check"></i><b>2.2.1</b> Quick Refresher on Logarithms</a></li>
<li class="chapter" data-level="2.2.2" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#log-transforming-variables"><i class="fa fa-check"></i><b>2.2.2</b> Log-Transforming Variables</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#fitting-the-regression-model"><i class="fa fa-check"></i><b>2.3</b> Fitting the Regression Model</a><ul>
<li class="chapter" data-level="2.3.1" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#examine-the-assumption-of-linearity"><i class="fa fa-check"></i><b>2.3.1</b> Examine the Assumption of Linearity</a></li>
<li class="chapter" data-level="2.3.2" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#interpret-the-regression-results"><i class="fa fa-check"></i><b>2.3.2</b> Interpret the Regression Results</a></li>
<li class="chapter" data-level="2.3.3" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#better-interpretations-back-transforming"><i class="fa fa-check"></i><b>2.3.3</b> Better Interpretations: Back-transforming</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#alternative-method-of-fitting-the-model"><i class="fa fa-check"></i><b>2.4</b> Alternative Method of Fitting the Model</a></li>
<li class="chapter" data-level="2.5" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#plotting-the-fitted-model"><i class="fa fa-check"></i><b>2.5</b> Plotting the Fitted Model</a></li>
<li class="chapter" data-level="2.6" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#different-base-values-in-the-logarithm"><i class="fa fa-check"></i><b>2.6</b> Different Base Values in the Logarithm</a><ul>
<li class="chapter" data-level="2.6.1" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#comparing-the-output-from-the-two-bases"><i class="fa fa-check"></i><b>2.6.1</b> Comparing the Output from the Two Bases</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#base-e-logarithm-the-natural-logarithm"><i class="fa fa-check"></i><b>2.7</b> Base-<span class="math inline">\(e\)</span> Logarithm: The Natural Logarithm</a><ul>
<li class="chapter" data-level="2.7.1" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#using-the-natural-logarithm-in-a-regression-model"><i class="fa fa-check"></i><b>2.7.1</b> Using the Natural Logarithm in a Regression Model</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#including-covariates"><i class="fa fa-check"></i><b>2.8</b> Including Covariates</a><ul>
<li class="chapter" data-level="2.8.1" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#plot-of-the-model-results"><i class="fa fa-check"></i><b>2.8.1</b> Plot of the Model Results</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#polynomial-effects-vs.-log-transformations"><i class="fa fa-check"></i><b>2.9</b> Polynomial Effects vs. Log-Transformations</a></li>
<li class="chapter" data-level="" data-path="nonlinearity-log-transforming-the-predictor.html"><a href="nonlinearity-log-transforming-the-predictor.html#other-resources-1"><i class="fa fa-check"></i>Other Resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html"><i class="fa fa-check"></i><b>3</b> Nonlinearity: Log-Transforming the Outcome</a><ul>
<li class="chapter" data-level="" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#preparation-2"><i class="fa fa-check"></i>Preparation</a></li>
<li class="chapter" data-level="3.1" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#dataset-and-research-question-1"><i class="fa fa-check"></i><b>3.1</b> Dataset and Research Question</a></li>
<li class="chapter" data-level="3.2" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#examine-relationship-between-age-and-budget"><i class="fa fa-check"></i><b>3.2</b> Examine Relationship between Age and Budget</a></li>
<li class="chapter" data-level="3.3" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#transform-the-outcome-using-the-natural-logarithm-base-e"><i class="fa fa-check"></i><b>3.3</b> Transform the Outcome Using the Natural Logarithm (Base-e)</a></li>
<li class="chapter" data-level="3.4" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#re-analyze-using-the-log-transformed-budget"><i class="fa fa-check"></i><b>3.4</b> Re-analyze using the Log-Transformed Budget</a></li>
<li class="chapter" data-level="3.5" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#interpreting-the-regression-output"><i class="fa fa-check"></i><b>3.5</b> Interpreting the Regression Output</a><ul>
<li class="chapter" data-level="3.5.1" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#back-transforming-a-more-useful-interpretation"><i class="fa fa-check"></i><b>3.5.1</b> Back-Transforming: A More Useful Interpretation</a></li>
<li class="chapter" data-level="3.5.2" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#substituting-in-values-for-age-to-interpret-effects"><i class="fa fa-check"></i><b>3.5.2</b> Substituting in Values for Age to Interpret Effects</a></li>
<li class="chapter" data-level="3.5.3" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#approximate-interpretation-of-the-slope"><i class="fa fa-check"></i><b>3.5.3</b> Approximate Interpretation of the Slope</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#plotting-the-fitted-model-1"><i class="fa fa-check"></i><b>3.6</b> Plotting the Fitted Model</a></li>
<li class="chapter" data-level="3.7" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#relationship-between-mpaa-rating-and-budget"><i class="fa fa-check"></i><b>3.7</b> Relationship between MPAA Rating and Budget</a><ul>
<li class="chapter" data-level="3.7.1" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#regression-model"><i class="fa fa-check"></i><b>3.7.1</b> Regression Model</a></li>
<li class="chapter" data-level="3.7.2" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#mathematical-explanation-1"><i class="fa fa-check"></i><b>3.7.2</b> Mathematical Explanation</a></li>
<li class="chapter" data-level="3.7.3" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#approximate-interpretations"><i class="fa fa-check"></i><b>3.7.3</b> Approximate Interpretations</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#multiple-regression-main-effects-model"><i class="fa fa-check"></i><b>3.8</b> Multiple Regression: Main Effects Model</a><ul>
<li class="chapter" data-level="3.8.1" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#nested-f-test"><i class="fa fa-check"></i><b>3.8.1</b> Nested F-Test</a></li>
<li class="chapter" data-level="3.8.2" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#coefficient-level-interpretation"><i class="fa fa-check"></i><b>3.8.2</b> Coefficient-Level Interpretation</a></li>
<li class="chapter" data-level="3.8.3" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#plot-of-the-fitted-model"><i class="fa fa-check"></i><b>3.8.3</b> Plot of the Fitted Model</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="nonlinearity-log-transforming-the-outcome.html"><a href="nonlinearity-log-transforming-the-outcome.html#multiple-regression-interaction-model"><i class="fa fa-check"></i><b>3.9</b> Multiple Regression: Interaction Model</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="log-transformations-some-final-thoughts.html"><a href="log-transformations-some-final-thoughts.html"><i class="fa fa-check"></i>Log Transformations: Some Final Thoughts</a><ul>
<li class="chapter" data-level="" data-path="log-transformations-some-final-thoughts.html"><a href="log-transformations-some-final-thoughts.html#power-transformations"><i class="fa fa-check"></i>Power Transformations</a><ul>
<li class="chapter" data-level="" data-path="log-transformations-some-final-thoughts.html"><a href="log-transformations-some-final-thoughts.html#ladder-of-transformations"><i class="fa fa-check"></i>Ladder of Transformations</a></li>
<li class="chapter" data-level="" data-path="log-transformations-some-final-thoughts.html"><a href="log-transformations-some-final-thoughts.html#rule-of-the-bulge"><i class="fa fa-check"></i>Rule of the Bulge</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="probability-distributions.html"><a href="probability-distributions.html"><i class="fa fa-check"></i><b>4</b> Probability Distributions</a><ul>
<li class="chapter" data-level="" data-path="probability-distributions.html"><a href="probability-distributions.html#preparation-3"><i class="fa fa-check"></i>Preparation</a></li>
<li class="chapter" data-level="4.1" data-path="probability-distributions.html"><a href="probability-distributions.html#dataset-and-research-question-2"><i class="fa fa-check"></i><b>4.1</b> Dataset and Research Question</a></li>
<li class="chapter" data-level="4.2" data-path="probability-distributions.html"><a href="probability-distributions.html#normal-distribution"><i class="fa fa-check"></i><b>4.2</b> Normal Distribution</a><ul>
<li class="chapter" data-level="4.2.1" data-path="probability-distributions.html"><a href="probability-distributions.html#other-useful-r-functions-for-working-with-probability-distributions"><i class="fa fa-check"></i><b>4.2.1</b> Other Useful R Functions for Working with Probability Distributions</a></li>
<li class="chapter" data-level="4.2.2" data-path="probability-distributions.html"><a href="probability-distributions.html#finding-cumulative-probability"><i class="fa fa-check"></i><b>4.2.2</b> Finding Cumulative Probability</a></li>
<li class="chapter" data-level="4.2.3" data-path="probability-distributions.html"><a href="probability-distributions.html#cumulative-density-and-p-value"><i class="fa fa-check"></i><b>4.2.3</b> Cumulative Density and <span class="math inline">\(p\)</span>-Value</a></li>
<li class="chapter" data-level="4.2.4" data-path="probability-distributions.html"><a href="probability-distributions.html#finding-quantiles"><i class="fa fa-check"></i><b>4.2.4</b> Finding Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="probability-distributions.html"><a href="probability-distributions.html#students-t-distribution"><i class="fa fa-check"></i><b>4.3</b> Student’s <span class="math inline">\(t\)</span>-Distribution</a><ul>
<li class="chapter" data-level="4.3.1" data-path="probability-distributions.html"><a href="probability-distributions.html#comparing-probability-densities"><i class="fa fa-check"></i><b>4.3.1</b> Comparing Probability Densities</a></li>
<li class="chapter" data-level="4.3.2" data-path="probability-distributions.html"><a href="probability-distributions.html#comparing-cumulative-densities"><i class="fa fa-check"></i><b>4.3.2</b> Comparing Cumulative Densities</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="probability-distributions.html"><a href="probability-distributions.html#using-the-t-distribution-in-regression"><i class="fa fa-check"></i><b>4.4</b> Using the <span class="math inline">\(t\)</span>-Distribution in Regression</a></li>
<li class="chapter" data-level="4.5" data-path="probability-distributions.html"><a href="probability-distributions.html#model-level-inference-the-f-distribution"><i class="fa fa-check"></i><b>4.5</b> Model-Level Inference: The <span class="math inline">\(F\)</span>-Distribution</a><ul>
<li class="chapter" data-level="4.5.1" data-path="probability-distributions.html"><a href="probability-distributions.html#testing-the-model-level-null-hypothesis"><i class="fa fa-check"></i><b>4.5.1</b> Testing the Model-Level Null Hypothesis</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="probability-distributions.html"><a href="probability-distributions.html#mean-squares-are-variance-estimates"><i class="fa fa-check"></i><b>4.6</b> Mean Squares are Variance Estimates</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html"><i class="fa fa-check"></i><b>5</b> Maximum Likelihood Estimation</a><ul>
<li class="chapter" data-level="" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#preparation-4"><i class="fa fa-check"></i>Preparation</a></li>
<li class="chapter" data-level="5.1" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#dataset-and-research-question-3"><i class="fa fa-check"></i><b>5.1</b> Dataset and Research Question</a></li>
<li class="chapter" data-level="5.2" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#joint-probability-density"><i class="fa fa-check"></i><b>5.2</b> Joint Probability Density</a></li>
<li class="chapter" data-level="5.3" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#likelihood"><i class="fa fa-check"></i><b>5.3</b> Likelihood</a></li>
<li class="chapter" data-level="5.4" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#maximum-likelihood"><i class="fa fa-check"></i><b>5.4</b> Maximum Likelihood</a><ul>
<li class="chapter" data-level="5.4.1" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#method-1-grid-search"><i class="fa fa-check"></i><b>5.4.1</b> Method 1: Grid Search</a></li>
<li class="chapter" data-level="5.4.2" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#log-likelihood"><i class="fa fa-check"></i><b>5.4.2</b> Log-Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#maximum-likelihood-estimation-for-regression"><i class="fa fa-check"></i><b>5.5</b> Maximum Likelihood Estimation for Regression</a><ul>
<li class="chapter" data-level="5.5.1" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#large-search-spaces"><i class="fa fa-check"></i><b>5.5.1</b> Large Search Spaces</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#ml-estimation-in-regression-using-r"><i class="fa fa-check"></i><b>5.6</b> ML Estimation in Regression Using R</a><ul>
<li class="chapter" data-level="5.6.1" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#using-r-to-directly-compute-the-likelihood-and-log-likelihood"><i class="fa fa-check"></i><b>5.6.1</b> Using R to Directly Compute the Likelihood and Log-Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="maximum-likelihood-estimation.html"><a href="maximum-likelihood-estimation.html#way-too-much-math"><i class="fa fa-check"></i><b>5.7</b> Way, Way, Way too Much Mathematics</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html"><i class="fa fa-check"></i><b>6</b> Information Criteria for Model Selection</a><ul>
<li class="chapter" data-level="" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html#preparation-5"><i class="fa fa-check"></i>Preparation</a></li>
<li class="chapter" data-level="6.1" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html#dataset-and-research-question-4"><i class="fa fa-check"></i><b>6.1</b> Dataset and Research Question</a></li>
<li class="chapter" data-level="6.2" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html#model-building"><i class="fa fa-check"></i><b>6.2</b> Model-Building</a><ul>
<li class="chapter" data-level="6.2.1" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html#exploration-of-the-outcome"><i class="fa fa-check"></i><b>6.2.1</b> Exploration of the Outcome</a></li>
<li class="chapter" data-level="6.2.2" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html#building-the-student-related-factors-model"><i class="fa fa-check"></i><b>6.2.2</b> Building the Student-Related Factors Model</a></li>
<li class="chapter" data-level="6.2.3" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html#building-the-faculty-related-factors-model"><i class="fa fa-check"></i><b>6.2.3</b> Building the Faculty-Related Factors Model</a></li>
<li class="chapter" data-level="6.2.4" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html#building-the-institution-related-factors-model"><i class="fa fa-check"></i><b>6.2.4</b> Building the Institution-Related Factors Model</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html#candidate-statistical-models"><i class="fa fa-check"></i><b>6.3</b> Candidate Statistical Models</a></li>
<li class="chapter" data-level="6.4" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html#log-likelihood-1"><i class="fa fa-check"></i><b>6.4</b> Log-Likelihood</a></li>
<li class="chapter" data-level="6.5" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html#deviance-an-alternative-fit-value"><i class="fa fa-check"></i><b>6.5</b> Deviance: An Alternative Fit Value</a></li>
<li class="chapter" data-level="6.6" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html#akiakes-information-criteria-aic"><i class="fa fa-check"></i><b>6.6</b> Akiake’s Information Criteria (AIC)</a></li>
<li class="chapter" data-level="6.7" data-path="information-criteria-for-model-selection.html"><a href="information-criteria-for-model-selection.html#empirical-support-for-hypotheses"><i class="fa fa-check"></i><b>6.7</b> Empirical Support for Hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="moreinfocrit.html"><a href="moreinfocrit.html"><i class="fa fa-check"></i><b>7</b> Model Evidence</a><ul>
<li class="chapter" data-level="" data-path="moreinfocrit.html"><a href="moreinfocrit.html#preparation-6"><i class="fa fa-check"></i>Preparation</a></li>
<li class="chapter" data-level="7.1" data-path="moreinfocrit.html"><a href="moreinfocrit.html#dataset-and-research-question-5"><i class="fa fa-check"></i><b>7.1</b> Dataset and Research Question</a></li>
<li class="chapter" data-level="7.2" data-path="moreinfocrit.html"><a href="moreinfocrit.html#corrected-aic-aicc-adjusting-for-model-complexity-and-sample-size"><i class="fa fa-check"></i><b>7.2</b> Corrected AIC (AICc): Adjusting for Model Complexity and Sample Size</a></li>
<li class="chapter" data-level="7.3" data-path="moreinfocrit.html"><a href="moreinfocrit.html#model-selection-uncertainty"><i class="fa fa-check"></i><b>7.3</b> Model-Selection Uncertainty</a></li>
<li class="chapter" data-level="7.4" data-path="moreinfocrit.html"><a href="moreinfocrit.html#relative-likelihood-and-evidence-ratios"><i class="fa fa-check"></i><b>7.4</b> Relative Likelihood and Evidence Ratios</a></li>
<li class="chapter" data-level="7.5" data-path="moreinfocrit.html"><a href="moreinfocrit.html#model-probabilities"><i class="fa fa-check"></i><b>7.5</b> Model Probabilities</a></li>
<li class="chapter" data-level="7.6" data-path="moreinfocrit.html"><a href="moreinfocrit.html#tables-of-model-evidence"><i class="fa fa-check"></i><b>7.6</b> Tables of Model Evidence</a></li>
<li class="chapter" data-level="7.7" data-path="moreinfocrit.html"><a href="moreinfocrit.html#some-final-thoughts"><i class="fa fa-check"></i><b>7.7</b> Some Final Thoughts</a></li>
<li class="chapter" data-level="7.8" data-path="moreinfocrit.html"><a href="moreinfocrit.html#pretty-printing-tables-of-model-evidence"><i class="fa fa-check"></i><b>7.8</b> Pretty Printing Tables of Model Evidence</a></li>
<li class="chapter" data-level="" data-path="moreinfocrit.html"><a href="moreinfocrit.html#other-resources-2"><i class="fa fa-check"></i>Other Resources</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="intro-lmer.html"><a href="intro-lmer.html"><i class="fa fa-check"></i><b>8</b> Introduction to Mixed-Effects Models</a><ul>
<li class="chapter" data-level="" data-path="intro-lmer.html"><a href="intro-lmer.html#preparation-7"><i class="fa fa-check"></i>Preparation</a></li>
<li class="chapter" data-level="8.1" data-path="intro-lmer.html"><a href="intro-lmer.html#dataset-and-research-question-6"><i class="fa fa-check"></i><b>8.1</b> Dataset and Research Question</a></li>
<li class="chapter" data-level="8.2" data-path="intro-lmer.html"><a href="intro-lmer.html#join-the-student--and-classroom-level-data"><i class="fa fa-check"></i><b>8.2</b> Join the Student- and Classroom-Level Data</a></li>
<li class="chapter" data-level="8.3" data-path="intro-lmer.html"><a href="intro-lmer.html#fixed-effects-regression-model"><i class="fa fa-check"></i><b>8.3</b> Fixed-Effects Regression Model</a><ul>
<li class="chapter" data-level="8.3.1" data-path="intro-lmer.html"><a href="intro-lmer.html#residual-analysis"><i class="fa fa-check"></i><b>8.3.1</b> Residual Analysis</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="intro-lmer.html"><a href="intro-lmer.html#conceptual-idea-of-mixed-effects-models"><i class="fa fa-check"></i><b>8.4</b> Conceptual Idea of Mixed-Effects Models</a></li>
<li class="chapter" data-level="8.5" data-path="intro-lmer.html"><a href="intro-lmer.html#fitting-the-mixed-effects-regression-model-in-practice"><i class="fa fa-check"></i><b>8.5</b> Fitting the Mixed-Effects Regression Model in Practice</a></li>
<li class="chapter" data-level="8.6" data-path="intro-lmer.html"><a href="intro-lmer.html#example-2-life-satisfaction-of-nba-players"><i class="fa fa-check"></i><b>8.6</b> Example 2: Life Satisfaction of NBA Players</a><ul>
<li class="chapter" data-level="8.6.1" data-path="intro-lmer.html"><a href="intro-lmer.html#fit-the-mixed-effects-model"><i class="fa fa-check"></i><b>8.6.1</b> Fit the Mixed-Effects Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lmer-cross-sectional.html"><a href="lmer-cross-sectional.html"><i class="fa fa-check"></i><b>9</b> Linear Mixed-Effects Models: Cross-Sectional Analysis</a><ul>
<li class="chapter" data-level="" data-path="lmer-cross-sectional.html"><a href="lmer-cross-sectional.html#preparation-8"><i class="fa fa-check"></i>Preparation</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="lmer-assumptions.html"><a href="lmer-assumptions.html"><i class="fa fa-check"></i><b>10</b> Linear Mixed-Effects Models: Alternative Representations and Assumptions</a><ul>
<li class="chapter" data-level="" data-path="lmer-assumptions.html"><a href="lmer-assumptions.html#preparation-9"><i class="fa fa-check"></i>Preparation</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="lmer-longitudinal.html"><a href="lmer-longitudinal.html"><i class="fa fa-check"></i><b>11</b> Linear Mixed-Effects Models: Longitudinal Analysis</a><ul>
<li class="chapter" data-level="" data-path="lmer-longitudinal.html"><a href="lmer-longitudinal.html#preparation-10"><i class="fa fa-check"></i>Preparation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data-codebook.html"><a href="data-codebook.html"><i class="fa fa-check"></i>Data Codebooks</a><ul>
<li class="chapter" data-level="" data-path="data-codebook.html"><a href="data-codebook.html#ed-schools-2018"><i class="fa fa-check"></i>ed-schools-2018.csv</a></li>
<li class="chapter" data-level="" data-path="data-codebook.html"><a href="data-codebook.html#evaluations"><i class="fa fa-check"></i>evaluations.csv</a></li>
<li class="chapter" data-level="" data-path="data-codebook.html"><a href="data-codebook.html#fci-2015"><i class="fa fa-check"></i>fci-2015.csv</a></li>
<li class="chapter" data-level="" data-path="data-codebook.html"><a href="data-codebook.html#mn-schools"><i class="fa fa-check"></i>mn-schools.csv</a></li>
<li class="chapter" data-level="" data-path="data-codebook.html"><a href="data-codebook.html#movies"><i class="fa fa-check"></i>movies.csv</a></li>
<li class="chapter" data-level="" data-path="data-codebook.html"><a href="data-codebook.html#nba"><i class="fa fa-check"></i>nba-player-data.csv and nba-team-data.csv</a></li>
<li class="chapter" data-level="" data-path="data-codebook.html"><a href="data-codebook.html#netherlands"><i class="fa fa-check"></i>netherlands-students.csv and netherlands-schools.csv</a></li>
<li class="chapter" data-level="" data-path="data-codebook.html"><a href="data-codebook.html#nhl"><i class="fa fa-check"></i>nhl.csv</a></li>
<li class="chapter" data-level="" data-path="data-codebook.html"><a href="data-codebook.html#popular"><i class="fa fa-check"></i>popular-classroom.csv and popular-student.csv</a></li>
<li class="chapter" data-level="" data-path="data-codebook.html"><a href="data-codebook.html#riverview"><i class="fa fa-check"></i>riverview.csv</a></li>
<li class="chapter" data-level="" data-path="data-codebook.html"><a href="data-codebook.html#vocabulary"><i class="fa fa-check"></i>vocabulary.csv</a></li>
<li class="chapter" data-level="" data-path="data-codebook.html"><a href="data-codebook.html#wine"><i class="fa fa-check"></i>wine.csv</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">EPsy 8252 Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lmer-longitudinal" class="section level1">
<h1><span class="header-section-number">Unit 11:</span> Linear Mixed-Effects Models: Longitudinal Analysis</h1>
<!-- ```{r echo=FALSE, message=FALSE} -->
<!-- library(knitr) -->
<!-- library(kableExtra) -->
<!-- opts_knit$set( -->
<!--   width = 85, -->
<!--   tibble.print_max = Inf -->
<!--   ) -->
<!-- opts_chunk$set( -->
<!--   prompt = FALSE, -->
<!--   comment = NA, -->
<!--   message = FALSE, -->
<!--   warning = FALSE, -->
<!--   tidy = FALSE, -->
<!--   fig.align = 'center', -->
<!--   fig.width = 6, -->
<!--   fig.height = 6, -->
<!--   out.width = '50%' -->
<!--   ) -->
<!-- ``` -->
<!-- In this set of notes, you will learn how to use the linear mixed-effects model to analyze longitudinal data. -->
<hr />
<div id="preparation-10" class="section level3 unnumbered">
<h3>Preparation</h3>
<p>Before class you will need to read:</p>
<ul>
<li>Uchikoshi, Y. (2005). <a href="http://psycnet.apa.org.ezp3.lib.umn.edu/journals/dev/41/3/464">Narrative development in bilingual kindergarteners: Can Arthur help?</a> <em>Developmental Psychology, 41</em>(3), 464–478. doi: 10.1037/0012-1649.41.3.464</li>
</ul>
<p><br /></p>
<hr />
<!-- ## Dataset and Research Question -->
<!-- In this set of notes, we will use data from the file *vocabulary.csv* (see the [data codebook](#vocabulary) here). These data include repeated measurements of scaled vocabulary scores for $n=64$ students. -->
<!-- ```{r message=FALSE, paged.print=FALSE} -->
<!-- # Load libraries -->
<!-- library(AICcmodavg) -->
<!-- library(broom) -->
<!-- library(dplyr) -->
<!-- library(ggplot2) -->
<!-- library(lme4) #for fitting mixed-effects models -->
<!-- library(readr) -->
<!-- library(sm) -->
<!-- library(tidyr) -->
<!-- # Read in data -->
<!-- vocabulary = read_csv(file = "~/Documents/github/epsy-8252/data/vocabulary.csv") -->
<!-- head(vocabulary) -->
<!-- ``` -->
<!-- We will use these data to explore the change in vocabulary over time (longitudinal variation in the vocabulary scores). We will focus on two primary research questions: (1) What is the growth pattern in the average vocabulary score over time? and (2) Is this growth pattern different for males and females? -->
<!-- ## Data Structure: Tidy/Long Data vs. Wide Data -->
<!-- Before doing any analysis of the data, it is worth understanding the structure of the data. There are two common structures for repeated measures data: *long/tidy structured data* and *wide structured data*.  -->
<!-- - In tidy/long structured data, there is a single column per variable. For example, the outcome variable (vocabulary scores) would be organized into a single column. Similarly, the predictor that designates time (grade-level in our example) would also be organized into a single column. -->
<!-- - In wide structured data, the outcome variable (or predictor variables) is typically spread out over multiple columns. Often there are not columns that include data on the time predictor; instead this information is typically embedded in the column name. -->
<!-- The vocabulary data is currently structured as wide data; the vocabulary scores are organized into four separate columns and the information about grade-level (the time predictor) is embedded in the variable names (e.g., `vocab_08` indicates 8th-grade). The same data are presented below in the tidy/long structure. -->
<!-- ```{r echo=FALSE} -->
<!-- vocabulary %>%  -->
<!--   gather(key = "grade", value = "vocab_score", vocab_08:vocab_11) %>% -->
<!--   arrange(id, grade) %>% -->
<!--   head(12) -->
<!-- ``` -->
<!-- Notice that in the tidy/long structured data that the vocabulary scores (outcome) are now organized into a single column. Grade-level (the time predictor) is also now explicitly included in the data and is also organized as a single column in the data. Note that in the long structure, each row now represents a particular student at a particular grade-level, and that each student's data now encompasses several rows. -->
<!-- There are advantages to each of the structures. For example the wide structure has the advantage of being a better structure for data entry. Since each row corresponds to a different student, there are fewer rows and therefore less redundancy in the data entry process. Compare this to the tidy/long data where each student's data encompasses four rows. If you were doing data entry in this structure you would need to record the student's sex four times rather than once in the wide structure. -->
<!-- The tidy/long structure is the structure that is needed for modeling. Thus, if one of the analytic goals is to fit a linear mixed-effects model to explain variation or examine predictor effects, the tidy/long data structure is key. Note that the wide structured data is also used in some analyses (e.g., computing correlations). -->
<!-- ### Switching between the Two Structures -->
<!-- The library **tidyr** has two functions, `gather()` (wide &rarr; tidy/long) and `spread()` (tidy/long &rarr; wide), that convert data between these two structures. Below, I show the code for going from the wide structured data (`vocabulary`) to the tidy/long structure. -->
<!-- ```{r echo=FALSE} -->
<!-- # Convert from wide to long structured data -->
<!-- vocabulary_long = vocabulary %>%  -->
<!--   gather(key = "grade", value = "vocab_score", vocab_08:vocab_11) %>% -->
<!--   arrange(id, grade) -->
<!-- # View data -->
<!-- head(vocabulary_long, 12) -->
<!-- ``` -->
<!-- For more infomation about using these functions, google "tidyr" and read through any number of great tutorials or vignettes; for example [here](http://data.library.virginia.edu/a-tidyr-tutorial/) or [here](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html). You can also read Hadley Wickham's original [paper on tidy data](http://vita.had.co.nz/papers/tidy-data.html). -->
<!-- ## Plot of the Mean and Individual Profiles -->
<!-- There are two plots that are particularly useful in exploring longitudinal data. The first is a plot of the mean value of the outcome at each time point (mean profile plot). This shows the average growth profile and is useful for determining the functional form of the fixed-effects part of the model; is the mean change over time linear? Quadratic? Log-linear? Another plot that is often examined is a spaghetti plot. A spaghetti plot shows the individual growth patterns or profiles and is useful for determining whether there is variation from the average profile. This helps us to consider the set of random-effects to include in the model. Below we examine both the mean profile and individual profiles simulateneously. -->
<!-- ```{r fig.width=6, fig.height=6, fig.cap='Plot showing the change in vocabulary score over time for 64 students. The average growth profile is also displayed.'} -->
<!-- ggplot(data = vocabulary_long, aes(x = grade, y = vocab_score)) + -->
<!--   geom_line(aes(group = id), alpha = 0.3) + -->
<!--   stat_summary(fun.y = mean, geom = "line", size = 2, group = 1) + -->
<!--   stat_summary(fun.y = mean, geom = "point", size = 3) + -->
<!--   theme_bw() + -->
<!--   scale_x_discrete(name = "Grade-level", labels = c("8th-grade", "9th-grade", "10th-grade", "11th-grade")) + -->
<!--   ylab("Vocabulary score") -->
<!-- ``` -->
<!-- Based on this plot: -->
<!-- - The average profile displays change over time that is positive (growth) and linear (or perhaps log-linear). -->
<!-- - The individual profiles show variation from the average profile; they have different vocabulary scores in 8th-grade and the profiles themselves vary in terms of their change (some show more change; others show decline) -->
<!-- ## Unconditional Random Intercepts Model -->
<!-- As in a cross-sectional analysis we begin a longitudinal analysis by fitting the unconditional random intercepts model. The statistical model in this example can be expressed as: -->
<!-- $$ -->
<!-- \mathrm{Vocabulary~Score}_{ij} = \big[\beta_0 + b_{0j}\big] + \epsilon_{ij} -->
<!-- $$ -->
<!-- where, -->
<!-- - $\mathrm{Vocabulary~Score}_{ij}$ is the vocabulary score at time point $i$ for student $j$; -->
<!-- - $\beta_0$ is the fixed-effect of intercept; -->
<!-- - $b_{0j}$ is the random-effect of intercept for student $j$; and -->
<!-- - $\epsilon_{ij}$ is the error at time point $i$ for student $j$.  -->
<!-- ```{r} -->
<!-- lmer.0 = lmer(vocab_score ~ 1 + (1|id), data = vocabulary_long, REML = FALSE) -->
<!-- summary(lmer.0) -->
<!-- ``` -->
<!-- Fitting the unconditional means model gives us our baseline comparison model. The variance components suggest that there is unexplained within-student variation ($\hat\sigma^2_{\epsilon}=1.83$) and unexplained between-student variation ($\hat\sigma^2_{\mathrm{ID}}=2.95$). Most of the unexplained variation seems to be between-student variation (61.8\%). -->
<!-- ## Unconditional Growth Model -->
<!-- We can now add the fixed-effect of time (the time predictor) to the model. In this data set, the time predictor is `grade`, which is a categorical predictor. We could create dummy variables, or simply add `grade` into the model and let R choose the reference group alphabetically (`vocab_08` in this example). The statistical model in this example can be expressed as: -->
<!-- $$ -->
<!-- \mathrm{Vocabulary~Score}_{ij} = \big[\beta_0 + b_{0j}\big] + \beta_1(\mathrm{9th\mbox{-}grade}) + \beta_2(\mathrm{10th\mbox{-}grade}) + \beta_3(\mathrm{11th\mbox{-}grade}) + \epsilon_{ij} -->
<!-- $$ -->
<!-- Fitting the model: -->
<!-- ```{r echo=FALSE} -->
<!-- lmer.1 = lmer(vocab_score ~ 1 + grade + (1|id), data = vocabulary_long, REML = FALSE) -->
<!-- summary(lmer.1) -->
<!-- ``` -->
<!-- The fitted equation is: -->
<!-- $$ -->
<!-- \hat{\mathrm{Vocabulary~Score}_{ij}} = 1.13 + 1.41(\mathrm{9th\mbox{-}grade}) + 1.86(\mathrm{10th\mbox{-}grade}) + 2.34(\mathrm{11th\mbox{-}grade}) -->
<!-- $$ -->
<!-- Interpreting the coefficients, -->
<!-- - The predicted average vocabulary score for 8th-grade students (intercept) is 1.13. -->
<!-- - On average, 9th-grade students have a vocabulary score that is 1.41-points higher than 8th-grade students. -->
<!-- - On average, 10th-grade students have a vocabulary score that is 1.86-points higher than 8th-grade students. -->
<!-- - On average, 11th-grade students have a vocabulary score that is 2.34-points higher than 8th-grade students. -->
<!-- Looking at the variance components: -->
<!-- - The model has explained 55.8\% of the within-student variation. This is because grade is a within-student predictor (it has values that vary within each student). -->
<!-- - The model has *increased* the variation between-students ($-$8.7\%). This is a mathematical artifact of the estimation process. -->
<!-- ### Likelihood Ratio Test: p-Values for Mixed-Effects Models -->
<!-- So long as the assumptions of the linear mixed-effects model have been met (see Assumptions notes), we can obtain a $p$-value for the effect of grade. The way we do this is by taking advanatage of the fact that the unconditional random intercepts model is a nested model of the unconditional growth model. If we have nested models, they can be compared using a *Likelihood Ratio Test*. To carry out this test, we use the `anova()` function and input the two mixed-effects models we want to compare. (Note: Both models need to be fitted with ML.) -->
<!-- ```{r} -->
<!-- anova(lmer.0, lmer.1) -->
<!-- ``` -->
<!-- The null hypothesis being tested is that the reduced model (`lmer.0`) and the full model (`lmer.1`) fit the data equally well. The way that we measure fit is via the deviance. The deviance of the reduced model is 1009.3 and that for the full model is 852.8. If the two models fit equally well, we would expect the difference in deviances to be zero. The actual difference in deviances is 156.46. (This is often referred to as $\Delta G^2$, for goodness-of-fit, or as $\chi^2$.) This indicates that the fuller model fits the sample data better than the reduced model; the full model has a smaller deviance. -->
<!-- As with any difference, we wonder whether this is within what would be expected because of sampling (chance) variation. To test this, we evaluate $\Delta G^2$ in a $\chi^2$-distribution with $df$ equal to the difference in $K$ between the two models ($K$ is the $df$ for each model). This difference should be the difference in the complexity between the two models; the difference in the estimated number of parameters. Our reduced model has three parameters being estimated ($\hat\beta_0$, $\hat\sigma^2_{\epsilon}$, and $\hat\sigma^2_{0}$), and our full model has six parameters being estimated ($\hat\beta_0$, $\hat\beta_{\mathrm{9th\mbox{-}grade}}$, $\hat\beta_{\mathrm{10th\mbox{-}grade}}$, $\hat\beta_{\mathrm{11th\mbox{-}grade}}$, $\hat\sigma^2_{\epsilon}$, and $\hat\sigma^2_{0}$). The difference in complexity between these models is $6-3 = 3$. -->
<!-- ```{r} -->
<!-- 1 - pchisq(156.46, df = 3) -->
<!-- ``` -->
<!-- Note that all of these results are given in the `anova()` output. This is typically reported as something like:  -->
<!-- > A likelihood ratio test indicated that the model that included the fixed-effects of grade-level fitted the data significantly better that the unconditional random intercepts model, $\chi^2(3) = 156.46$, $p < .001$. -->
<!-- Why is this called a likelihood ratio test? Remember that the deviance is equal to $-2\mathrm{\ln(\mathcal{L})}$. Thus the difference in deviances can be written as: -->
<!-- $$ -->
<!-- \Delta G^2 = -2 \ln\big[\mathcal{L}(\mathrm{Reduced~Model})\big] - \bigg[-2 \ln\big[\mathcal{L}(\mathrm{Full~Model})\big]\bigg] -->
<!-- $$ -->
<!-- Pulling out the $-2$ we get -->
<!-- $$ -->
<!-- \Delta G^2 = -2 \bigg[\ln\big[\mathcal{L}(\mathrm{Reduced~Model})\big] - \ln\big[\mathcal{L}(\mathrm{Full~Model})\big]\bigg] -->
<!-- $$ -->
<!-- The difference between two logarithms, e.g., $\log(A)-\log(B)$ is the logarithm of the quotient ($\log(\frac{A}{B})$). Thus, we can re-write this as, -->
<!-- $$ -->
<!-- \Delta G^2 = -2 \ln \bigg[\frac{\mathcal{L}(\mathrm{Reduced~Model})}{\mathcal{L}(\mathrm{Full~Model}}\bigg] -->
<!-- $$ -->
<!-- Now it should be a little more apparent why this test is called a likelihood ratio test. Note that if both models fit the data equally well, their likelihood values would be equivalent and thus this equation would reduce to: -->
<!-- $$ -->
<!-- \begin{split} -->
<!-- \Delta G^2 &= -2 \ln \bigg[1\bigg] \\ -->
<!-- &= -2(0) \\ -->
<!-- &= 0 -->
<!-- \end{split} -->
<!-- $$ -->
<!-- Thus if the difference in the goodness-of-fit between the two models turns out to be zero (or are within chance variation of zero), both models fit the data equally and thus we should adopt the reduced model (Occam's Razor). -->
<!-- ## Quantitative Time Predictor: A More Flexible Model for Repeated Measures Data -->
<!-- One advantage to using the linear mixed-effects model to analyze repeated measures data over traditional methods (e.g., RM-ANOVA or MANOVA) is that the regression model allows for both categorical and quantitative variables. For example, rather than code our grade-levels categorically (as `vocab_08`, `vocab_09`, `vocab_10` and `vocab_11`), which was a necessity in days of yore, we could have simply coded them as 8, 9, 10, and 11. Then we could have fitted the LME model using this quantitative predictor.  -->
<!-- To convert `grade` to a quantitative variable, we create a lookup table which maps the levels of the categorical time predictor to the values we want to use in our new quantitative predictor. Below I show this mapping for two quantitative predictors, `grade_quant` which is a straight mapping to the relevant grade-level and `grade_quant_center` which centers the `grade_quant` predictor by subtracting 8 from each value. -->
<!-- \newpage -->
<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- # Create lookup table -->
<!-- lookup_table = data.frame( -->
<!--   grade = c("vocab_08", "vocab_09", "vocab_10", "vocab_11"), -->
<!--   grade_quant = c(8, 9, 10, 11), -->
<!--   grade_quant_center = c(0, 1, 2, 3) -->
<!-- ) -->
<!-- # View lookup table -->
<!-- lookup_table -->
<!-- ``` -->
<!-- Then, we join the tidy/long data with the lookup table. -->
<!-- ```{r message=FALSE, warning=FALSE} -->
<!-- vocabulary_long_2 = left_join(vocabulary_long, lookup_table, by = "grade") -->
<!-- head(vocabulary_long_2) -->
<!-- ``` -->
<!-- Below we fit the LME model using the `grade_quant` predictor. -->
<!-- ```{r} -->
<!-- lmer.2 = lmer(vocab_score ~ 1 + grade_quant + (1|id), data = vocabulary_long_2, REML = FALSE) -->
<!-- summary(lmer.2) -->
<!-- ``` -->
<!-- The fitted equation is: -->
<!-- $$ -->
<!-- \hat{\mathrm{Vocabulary~Score}_{ij}} = -4.56 + 0.75(\mathrm{Grade\mbox{-}level}_{ij}) -->
<!-- $$ -->
<!-- The model using the quantitative predictor of grade-level is simpler than the model using the categorical version of grade-level since it has two fewer fixed-effects to estimate (fewer model degrees-of-freedom). -->
<!-- Interpreting the coefficients, -->
<!-- - The predicted average vocabulary score for 0th-grade students (intercept) is -4.55 (extrapolation). -->
<!-- - Each one-unit difference in grade-level is associated with a 0.75-point difference in vocabulary score, on average. -->
<!-- Looking at the variance components: -->
<!-- - The model has explained 50.8\% of the within-student variation. -->
<!-- - The model has *increased* the variation between-students ($-$7.8\%). This is a mathematical artifact of the estimation process. -->
<!-- This is similar to the variance components obtained from the model using the categorical predictors.  -->
<!-- ### Centered Time Predictor -->
<!-- Let's fit the model using the centered quantitative predictor. -->
<!-- ```{r} -->
<!-- lmer.3 = lmer(vocab_score ~ 1 + grade_quant_center + (1|id), data = vocabulary_long_2, REML = FALSE) -->
<!-- summary(lmer.3) -->
<!-- ``` -->
<!-- The fitted equation is: -->
<!-- $$ -->
<!-- \hat{\mathrm{Vocabulary~Score}_{ij}} = 1.41 + 0.75(\mathrm{Centered~grade\mbox{-}level}_{ij}) -->
<!-- $$ -->
<!-- Interpreting the coefficients, -->
<!-- - The predicted average vocabulary score for 8th-grade students is 1.41. Centering removes the problem of extrapolation in the interpretation because we have now made 0 a legitimate value in the predictor. -->
<!-- - Each one-unit difference in grade-level is associated with a 0.75-point difference in vocabulary score, on average. This is identical to the previous model since we have not changed what a one-unit difference in the predictor represents. -->
<!-- Looking at the variance components: -->
<!-- - The model has explained 50.8\% of the within-student variation. -->
<!-- - The model has *increased* the variation between-students ($-$7.8\%). This is a mathematical artifact of the estimation process. -->
<!-- These values are identical to the variance components obtained from the previous model. -->
<!-- We can see why the intercepts are different but the slopes are the same by comparing the plots of the individual growth profiles and the fitted fixed-effects models for the two predictors. -->
<!-- ```{r fig.width=8, fig.height=4, fig.cap='Plot showing the change in vocabulary score over time for 64 students. The average growth profile is also displayed. This is shown for the non-centered (left) and 8th-grade centered (right) grade-level. A large blue point is shown at the intercept value in both plots.', out.width='80%'} -->
<!-- p1 = ggplot(data = vocabulary_long_2, aes(x = grade_quant, y = vocab_score)) + -->
<!--   geom_line(aes(group = id), alpha = 0.3) + -->
<!--   geom_abline(intercept = -4.56, slope = 0.75, color = "blue") + -->
<!--   geom_point(x = 0, y = -4.56, size = 1.5, color = "blue") + -->
<!--   theme_bw() + -->
<!--   scale_x_continuous(name = "Grade-level", limits = c(0, 11), breaks = c(0, 2, 4, 6, 8, 10)) + -->
<!--   scale_y_continuous(name = "Vocabulary score", limits = c(-5, 10)) -->
<!-- p2 = ggplot(data = vocabulary_long_2, aes(x = grade_quant_center, y = vocab_score)) + -->
<!--   geom_line(aes(group = id), alpha = 0.3) + -->
<!--   geom_abline(intercept = 1.41, slope = 0.75, color = "blue") + -->
<!--   geom_point(x = 0, y = 1.41, size = 1.5, color = "blue") + -->
<!--   theme_bw() + -->
<!--   scale_x_continuous(name = "Centered grade-level (0 = 8th grade)", limits = c(-8, 3), breaks = 0:4) + -->
<!--   ylab("Vocabulary score") -->
<!-- gridExtra::grid.arrange(p1, p2, nrow = 1) -->
<!-- ``` -->
<!-- Because of the interpretive value of the intercept when we center the grade-level predictor, we will fit all future models using the 8th-grade centered grade-level. -->
<!-- ## Examining the Functional Form of the Growth Model -->
<!-- As in any regression analysis. we need to specify the functional form of the growth model. Below we consider three potential functional forms between grade-level and vocabulary score: (1) a linear relationship (`lmer.3`); (2) a quadratic relatinship; and (3) a log-linear relationship (based on log-transforming grade-level). -->
<!-- ```{r} -->
<!-- # Quadratic model -->
<!-- lmer.4 = lmer(vocab_score ~ 1 + grade_quant_center + I(grade_quant_center^2) + (1|id), data = vocabulary_long_2, REML = FALSE) -->
<!-- # Log-linear model -->
<!-- lmer.5 = lmer(vocab_score ~ 1 + log(grade_quant_center + 1) + (1|id), data = vocabulary_long_2, REML = FALSE) -->
<!-- # Model-evidence -->
<!-- aictab( -->
<!--   cand.set = list(lmer.3, lmer.4, lmer.5), -->
<!--   modnames = c("Linear", "Quadratic", "Log-linear") -->
<!-- ) -->
<!-- ``` -->
<!-- Given the data and candidate models, the evidence supports the log-linear model. There is some slight evidence for the quadratic model and almost no evidence for the linear model. This is consistent with the nonlinearity we observed in the mean profile earlier. We should also evaluate the residuals for both models.  -->
<!-- ```{r out.width='31%', fig.show='hold', fig.cap='Plots of the Level-1 residuals and random-effects for the log-linear model (Row 1) and the quadratic model (Row 2).'} -->
<!-- out_4 = augment(lmer.4) -->
<!-- out_5 = augment(lmer.5) -->
<!-- # Log-linear model -->
<!-- ggplot(data = out_5, aes(x = .fitted, y = .resid)) + -->
<!--   geom_point() + -->
<!--   geom_hline(yintercept = 0) + -->
<!--   theme_bw() + -->
<!--   xlab("Fitted values") + -->
<!--   ylab("Level-1 residuals") + -->
<!--   ggtitle("Log-linear") -->
<!-- sm.density(out_5$.resid, model = "normal", main = "Main-Effect", xlab = "Level-1 residuals") -->
<!-- sm.density(ranef(lmer.5)$id[ , 1], model = "normal", xlab = "Random effects of the intercept") -->
<!-- # Quadratic model -->
<!-- ggplot(data = out_4, aes(x = .fitted, y = .resid)) + -->
<!--   geom_point() + -->
<!--   geom_hline(yintercept = 0) + -->
<!--   theme_bw() + -->
<!--   xlab("Fitted values") + -->
<!--   ylab("Level-1 residuals") + -->
<!--   ggtitle("Quadratic") -->
<!-- sm.density(out_4$.resid, model = "normal", main = "Interaction Effect", xlab = "Level-1 residuals") -->
<!-- sm.density(ranef(lmer.4)$id[ , 1], model = "normal", xlab = "Random effects of the intercept") -->
<!-- ``` -->
<!-- The residual plots look similar indicating that neither model meets the assumptions better than the other. Given this, the higher evidence for the log-linear model, and the simplicity of the log-linear model relative to the quadratic model, we will adopt the log-linear growth profile. -->
<!-- ```{r} -->
<!-- summary(lmer.5) -->
<!-- ``` -->
<!-- The fitted equation is: -->
<!-- $$ -->
<!-- \hat{\mathrm{Vocabulary~Score}_{ij}} = 1.21 + 1.66\bigg[\ln(\mathrm{Centered~grade\mbox{-}level}_{ij}+1)\bigg] -->
<!-- $$ -->
<!-- Interpreting the coefficients, -->
<!-- - The predicted average vocabulary score for 8th-grade students is 1.21. -->
<!-- - Each one-percent difference in grade-level is associated with a 0.0166-point difference in vocabulary score, on average. -->
<!-- Looking at the variance components: -->
<!-- - The model has explained 54.9\% of the within-student variation. -->
<!-- - The model has *increased* the variation between-students ($-$8.4\%). This is a mathematical artifact of the estimation process. -->
<!-- These values are quite similar to the variance components obtained from the previous model.  -->
<!-- ### Plot of the Fixed-Effects Model -->
<!-- The *t*-value associated with the fixed-effect of grade-level ($t=15.31$) indicates that the log-linear relationship between grade-level and vocabulary score is statistically important. To better understand this relationship we can plot the fixed-effects part of the unconditional growth model. -->
<!-- ```{r fig.width=6, fig.height=6, fig.cap='Predicted change in vocabulary score as a function of grade-level.'} -->
<!-- # Set up data -->
<!-- plot_data = crossing( -->
<!--   grade_quant_center = seq(from = 0, to = 3, by = 0.01) -->
<!-- ) %>% -->
<!--   mutate( -->
<!--     yhat = predict(lmer.5, newdata = ., re.form = NA) -->
<!--   ) -->
<!-- head(plot_data) -->
<!-- # Create plot -->
<!-- ggplot(data = plot_data, aes(x = grade_quant_center, y = yhat)) + -->
<!--   geom_line() + -->
<!--   theme_bw() + -->
<!--   scale_x_continuous(name = "Grade-level", breaks = c(0, 1, 2, 3), labels = c(8, 9, 10, 11)) + -->
<!--   ylab("Vocabulary score") -->
<!-- ``` -->
<!-- Based on this and the coefficient-level output, we can answer the first research question. The growth pattern in vocabulary is log-linear over time. While the change in vocabulary score, on average, is positive, the growth rate somewhat diminishes over time.  -->
<!-- ## Examining the Male and Female Growth Profiles -->
<!-- To answer the second research question about whether the growth pattern is different for males and females, we again plot the individual and mean growth profiles for females and males. -->
<!-- ```{r fig.width=8, fig.height=4, fig.cap='Plot showing the change in vocabulary score over time conditioned on sex. The average growth profile is also displayed for each sex.'} -->
<!-- # Turn female into factor for better plotting -->
<!-- vocabulary_long_2 %>% -->
<!--   mutate( -->
<!--     Sex = factor(female, levels = c(0, 1), labels = c("Male", "Female")) -->
<!--   ) %>% -->
<!-- ggplot(aes(x = grade_quant, y = vocab_score, color = Sex)) + -->
<!--   geom_line(aes(group = id), alpha = 0.3) + -->
<!--   stat_summary(fun.y = mean, geom = "line", size = 2, group = 1) + -->
<!--   stat_summary(fun.y = mean, geom = "point", size = 3) + -->
<!--   theme_bw() + -->
<!--   xlab("Grade-level") + -->
<!--   ylab("Vocabulary score") + -->
<!--   facet_wrap(~Sex) + -->
<!--   ggsci::scale_color_d3() -->
<!-- ``` -->
<!-- The plot suggests that the females tend to have higher average vocabulary scores than males at each grade level. The sample average growth profiles also show slightly different patterns of growth between males and females. To examine whether this is due to chance, we can fit models that include fixed-effects of sex. -->
<!-- If there is a main-effect of `female`, it will allow us to conclude that the growth pattern is the same, but that the average females vocabulary score is systematically different than that for males at each time point (e.g., always lower or higher by the same amount). If there is an interaction-effect between `female` and grade-level, it will allow us to conclude that the pattern of change over time is different between males and females. -->
<!-- Since the unconditional growth model is nested in the growth model with the main-effect of `female` and that, in turn, is nested inside the interaction model, we can use a series of likelihood ratio tests to examine these models. -->
<!-- ```{r} -->
<!-- # Main-effect of sex -->
<!-- lmer.6 = lmer(vocab_score ~ 1 + log(grade_quant_center + 1) + female +  -->
<!--                 (1|id), data = vocabulary_long_2, REML = FALSE) -->
<!-- # Interaction-effect between sex and grade-level -->
<!-- lmer.7 = lmer(vocab_score ~ 1 + log(grade_quant_center + 1) + female + log(grade_quant_center + 1):female + -->
<!--                 (1|id), data = vocabulary_long_2, REML = FALSE) -->
<!-- # LRT -->
<!-- anova(lmer.5, lmer.6, lmer.7) -->
<!-- ``` -->
<!-- The first $\chi^2$-test (on the `lmer.6` line) compares the growth model that includes the main-effect of sex to the unconditional growth model (the model on the previous line). This result is statistically significant, $\chi^2(1)=43.62$, $p<0.001$. This suggests that the model that includes the main-effect of sex has significantly less error than the unconditional growth model. -->
<!-- The second $\chi^2$-test (on the `lmer.7` line) compares the growth model that includes the interaction-effect between sex and grade-level to the growth model that includes the main-effect of sex. This result is not statistically significant, $\chi^2(1)=1.49$, $p=0.222$. This suggests that the model that includes the interaction-effect between sex and grade-level does not have significantly less error than the model that includes the main-effect of sex.  -->
<!-- This series of tests suggests that of these models, we should adopt the model that includes the main-effect of sex. We should also examine a table of model evidence. -->
<!-- ```{r} -->
<!-- # Model-evidence -->
<!-- aictab( -->
<!--   cand.set = list(lmer.5, lmer.6, lmer.7), -->
<!--   modnames = c("Unconditional growth", "Main-effect of sex", "Interaction-effect") -->
<!-- ) -->
<!-- ``` -->
<!-- Here the evidence slightly favors the model that includes the main-effect of sex. There is also a fair bit of evidence to support the interaction model. Again, we should probably examine the residuals from both of these models and adopt the model that better meets the assumptions. -->
<!-- ```{r out.width='31%', fig.show='hold', fig.cap='Plots of the Level-1 residuals and random-effects for the main-effects model (Row 1) and the interaction model (Row 2).'} -->
<!-- out_6 = augment(lmer.6) -->
<!-- out_7 = augment(lmer.7) -->
<!-- # Main-effect model -->
<!-- ggplot(data = out_6, aes(x = .fitted, y = .resid)) + -->
<!--   geom_point() + -->
<!--   geom_hline(yintercept = 0) + -->
<!--   theme_bw() + -->
<!--   xlab("Fitted values") + -->
<!--   ylab("Level-1 residuals") + -->
<!--   ggtitle("Main-Effect") -->
<!-- sm.density(out_6$.resid, model = "normal", main = "Main-Effect", xlab = "Level-1 residuals") -->
<!-- sm.density(ranef(lmer.6)$id[ , 1], model = "normal", xlab = "Random effects of the intercept") -->
<!-- # Interaction model -->
<!-- ggplot(data = out_7, aes(x = .fitted, y = .resid)) + -->
<!--   geom_point() + -->
<!--   geom_hline(yintercept = 0) + -->
<!--   theme_bw() + -->
<!--   xlab("Fitted values") + -->
<!--   ylab("Level-1 residuals") + -->
<!--   ggtitle("Interaction Effect") -->
<!-- sm.density(out_7$.resid, model = "normal", main = "Interaction Effect", xlab = "Level-1 residuals") -->
<!-- sm.density(ranef(lmer.7)$id[ , 1], model = "normal", xlab = "Random effects of the intercept") -->
<!-- ``` -->
<!-- The residuals for both models look fairly reasonable. Neither model shows improved fit based on these plots. For now, since there is evidence to support both models, we will interpret the coefficients and variance components from each of these models, and also plot their fixed-effects. -->
<!-- ### Main-Effect of Sex -->
<!-- ```{r} -->
<!-- summary(lmer.6) -->
<!-- ``` -->
<!-- The fitted equation is: -->
<!-- $$ -->
<!-- \hat{\mathrm{Vocabulary~Score}_{ij}} = -0.01 + 1.67\bigg[\ln(\mathrm{Centered~grade\mbox{-}level}_{ij}+1)\bigg] + 2.60(\mathrm{Female}_{\boldsymbol{\cdot}j}) -->
<!-- $$ -->
<!-- Interpreting the coefficients, -->
<!-- - The predicted average vocabulary score for male 8th-grade students is -0.01. -->
<!-- - Each one-percent difference in grade-level is associated with a 0.0167-point difference in vocabulary score, on average, controlling for differences in sex. -->
<!-- - Females have an average vocabulary score that is 2.60-points higher than males, controlling for differences in grade-level. -->
<!-- Looking at the variance components: -->
<!-- - The model has explained 54.9\% of the within-student variation. -->
<!-- - The model has explained 16.8\% of the between-student variation.  -->
<!-- We expect the model to explain variation between-students as the `female` predictor we included was a between-students predictor. Plotting this model (see below) we find that the growth pattern in vocabulary is log-linear over time for both females and males. While the change in vocabulary score, on average, is positive for both sexes, the growth rate somewhat diminishes over time. Moreover, while females tend to have a higher vocabulary score at each grade level, the change patterns seem to have the same rate of growth for both sexes.  -->
<!-- ```{r fig.width=6, fig.height=6, fig.cap='Predicted change in vocabulary score as a function of grade-level and sex.'} -->
<!-- # Set up data -->
<!-- plot_data = crossing( -->
<!--   grade_quant_center = seq(from = 0, to = 3, by = 0.01), -->
<!--   female = c(0, 1) -->
<!-- ) %>% -->
<!--   mutate( -->
<!--     yhat = predict(lmer.6, newdata = ., re.form = NA), -->
<!--     Sex = factor(female, levels = c(0, 1), labels = c("Male", "Female")) -->
<!--   ) -->
<!-- head(plot_data) -->
<!-- # Create plot -->
<!-- ggplot(data = plot_data, aes(x = grade_quant_center, y = yhat, color = Sex, linetype = Sex)) + -->
<!--   geom_line() + -->
<!--   theme_bw() + -->
<!--   scale_x_continuous(name = "Grade-level", breaks = c(0, 1, 2, 3), labels = c(8, 9, 10, 11)) + -->
<!--   ylab("Vocabulary score") + -->
<!--   ggsci::scale_color_d3() -->
<!-- ``` -->
<!-- ### Interaction-Effect between Sex and Grade-Level -->
<!-- ```{r} -->
<!-- summary(lmer.7) -->
<!-- ``` -->
<!-- The fitted equation is: -->
<!-- $$ -->
<!-- \hat{\mathrm{Vocabulary~Score}_{ij}} = -0.11 + 1.79\bigg[\ln(\mathrm{Centered~grade\mbox{-}level}_{ij}+1)\bigg] + 2.81(\mathrm{Female}_{\boldsymbol{\cdot}j}) - 0.27\bigg[\ln(\mathrm{Centered~grade\mbox{-}level}_{ij}+1)\times(\mathrm{Female}_{\boldsymbol{\cdot}j})\bigg] -->
<!-- $$ -->
<!-- Interpreting the coefficients , -->
<!-- - The predicted average vocabulary score for male 8th-grade students is -0.11. -->
<!-- - For males, each one-percent difference in grade-level is associated with a 0.0179-point difference in vocabulary score, on average. -->
<!-- - Eighth-grade females have an average vocabulary score that is 2.81-points higher than 8th-grade males. -->
<!-- - For females, each one-percent difference in grade-level is associated with a 0.0152-point difference in vocabulary score, on average. This is less than the effect for males by 0.0027. -->
<!-- More generally, we might say: -->
<!-- - The effect of grade-level differs by sex. -->
<!-- - The effect of sex differs by grade-level -->
<!-- Looking at the variance components: -->
<!-- - The model has explained 55.3\% of the within-student variation. -->
<!-- - The model has explained 16.7\% of the between-student variation.  -->
<!-- Plotting this model (see below; syntax not displayed) we find that the growth pattern in vocabulary is log-linear over time for both females and males. While the change in vocabulary score, on average, is positive for both sexes, the growth rate somewhat diminishes over time. Moreover, while females tend to have a higher vocabulary score at each grade level, the growth rate for females is slightly smaller than that for males.  -->
<!-- ```{r fig.width=6, fig.height=6, fig.cap='Predicted change in vocabulary score as a function of grade-level.', echo=FALSE} -->
<!-- # Set up data -->
<!-- plot_data_2 = crossing( -->
<!--   grade_quant_center = seq(from = 0, to = 3, by = 0.01), -->
<!--   female = c(0, 1) -->
<!-- ) %>% -->
<!--   mutate( -->
<!--     yhat = predict(lmer.7, newdata = ., re.form = NA), -->
<!--     Sex = factor(female, levels = c(0, 1), labels = c("Male", "Female")) -->
<!--   ) -->
<!-- head(plot_data_2) -->
<!-- # Create plot -->
<!-- ggplot(data = plot_data_2, aes(x = grade_quant_center, y = yhat, color = Sex, linetype = Sex)) + -->
<!--   geom_line() + -->
<!--   theme_bw() + -->
<!--   scale_x_continuous(name = "Grade-level", breaks = c(0, 1, 2, 3), labels = c(8, 9, 10, 11)) + -->
<!--   ylab("Vocabulary score") + -->
<!--   ggsci::scale_color_d3() -->
<!-- ``` -->
<!-- ## Other Resources {-} -->
<!-- In addition to the notes and what we cover in class, there many other resources for learning about using linear mixed-effects models for longitudinal analysis. Here are some resources that may be helpful in that endeavor: -->
<!-- - Long, J. D. (2012). [Longitudinal data analysis for the behavioral sciences using R](http://www.amazon.com/Longitudinal-Analysis-Behavioral-Sciences-Using/dp/1412982685). Thousand Oaks, CA: Sage. -->
<!-- - Swihart, B. J., Caffo, B., James, B. D., Strand, M., Schwartz, B. S., &amp; Punjabi, N. M. (2010). [Lasagna plots: A saucy alternative to spaghetti plots.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2937254/) *Epidemiology, 21*(5), 621&ndash;625. -->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lmer-assumptions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="data-codebook.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": {},
"instapper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"collapse": "section",
"toolbar": null,
"position": "fixed",
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
